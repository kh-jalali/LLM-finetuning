{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a03430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\windows10\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\windows10\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:740: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9020). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ..\\c10\\cuda\\CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import transformers\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "global_config = None\n",
    "\n",
    "#############################\n",
    "########## Permissions ##########\n",
    "#############################\n",
    "model_name_to_id = {\n",
    "  \"bigger_model_name\" : \"06ad41e68cd839fb475a0c1a4ee7a3ad398228df01c9396a97788295d5a0f8bb\"\n",
    "}\n",
    "\n",
    "#############################\n",
    "########## LOGGING ##########\n",
    "#############################\n",
    "def initialize_config_and_logging(existing_config=None):\n",
    "    global global_config\n",
    "    global_config = build_config(existing_config)\n",
    "    setup_logging(global_config)\n",
    "    logger.debug(\"Config: \" + str(yaml.dump(global_config.as_dict())))\n",
    "    return global_config\n",
    "\n",
    "def get_config():\n",
    "    global global_config\n",
    "    assert global_config is not None\n",
    "    return global_config\n",
    "\n",
    "def build_config(existing_config=None):\n",
    "    configs = [\n",
    "        # Using config library\n",
    "        config.config_from_env(prefix=\"LLAMA\", separator=\"_\", lowercase_keys=True),\n",
    "    ]\n",
    "\n",
    "    if existing_config:\n",
    "        if isinstance(existing_config, dict):\n",
    "            configs.append(config.config_from_dict(existing_config))\n",
    "        else:\n",
    "            configs.append(existing_config)\n",
    "\n",
    "    config_paths = get_config_paths()\n",
    "\n",
    "    for path in reversed(config_paths):\n",
    "        print(\"Loading builtin config from \" + path)\n",
    "        configs.append(config.config_from_yaml(path, read_from_file=True))\n",
    "\n",
    "    return config.ConfigurationSet(*configs)\n",
    "\n",
    "def get_config_paths():\n",
    "    paths = []\n",
    "\n",
    "def get_config_paths():\n",
    "    paths = []\n",
    "\n",
    "    config_name = \"llama_config\"\n",
    "    config_base = \"configs\"\n",
    "\n",
    "    base_config_path = os.path.join(config_base, config_name + \".yaml\")\n",
    "    if os.path.exists(base_config_path):\n",
    "        paths.append(base_config_path)\n",
    "\n",
    "    local_config_path = os.path.join(config_base, config_name + \"_local.yaml\")\n",
    "    if os.path.exists(local_config_path):\n",
    "        paths.append(local_config_path)\n",
    "\n",
    "    home = os.path.expanduser(\"~\")\n",
    "    home_config_path = os.path.join(home, \".\" + config_name + \".yaml\")\n",
    "    if os.path.exists(home_config_path):\n",
    "        paths.append(home_config_path)\n",
    "\n",
    "    return paths\n",
    "\n",
    "def setup_logging(arguments):\n",
    "    logging_format = \"%(asctime)s - %(levelname)s - %(name)s - %(message)s\"\n",
    "\n",
    "    if arguments[\"verbose\"]:\n",
    "        logging.basicConfig(level=logging.DEBUG, format=logging_format)\n",
    "    elif arguments[\"verbose_info\"]:\n",
    "        logging.basicConfig(level=logging.INFO, format=logging_format)\n",
    "    else:\n",
    "        logging.basicConfig(level=logging.WARNING, format=logging_format)\n",
    "\n",
    "    root_logger = logging.getLogger()\n",
    "\n",
    "    if arguments[\"verbose\"]:\n",
    "        root_logger.setLevel(logging.DEBUG)\n",
    "    elif arguments[\"verbose_info\"]:\n",
    "        root_logger.setLevel(logging.INFO)\n",
    "    else:\n",
    "        root_logger.setLevel(logging.WARNING)\n",
    "\n",
    "    logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"filelock\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"smart_open\").setLevel(logging.WARNING)\n",
    "    logging.getLogger(\"botocore\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "##########################\n",
    "########## DATA ##########\n",
    "##########################\n",
    "# Wrapper for data load, split, tokenize for training\n",
    "def tokenize_and_split_data(training_config, tokenizer):\n",
    "  initialized_config = initialize_config_and_logging(training_config)\n",
    "  dataset_path = initialized_config[\"datasets\"][\"path\"]\n",
    "  use_hf = initialized_config[\"datasets\"][\"use_hf\"]\n",
    "  print(\"tokenize\", use_hf, dataset_path)\n",
    "  if use_hf:\n",
    "    dataset = datasets.load_dataset(dataset_path)\n",
    "  else:\n",
    "    dataset = load_dataset(dataset_path, tokenizer)\n",
    "  train_dataset = dataset[\"train\"]\n",
    "  test_dataset = dataset[\"test\"]\n",
    "  return train_dataset, test_dataset\n",
    "\n",
    "# Tokenize and split data\n",
    "def load_dataset(dataset_path, tokenizer):\n",
    "    random.seed(42)\n",
    "    finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    max_length = training_config[\"model\"][\"max_length\"]\n",
    "    tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "        get_tokenize_function(tokenizer, max_length), # returns tokenize_function\n",
    "        batched=True,\n",
    "        batch_size=1,\n",
    "        drop_last_batch=True\n",
    "    )\n",
    "    tokenized_dataset = tokenized_dataset.with_format(\"torch\")\n",
    "    split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
    "    return split_dataset\n",
    "\n",
    "# Get function for tokenization, based on config parameters\n",
    "def get_tokenize_function(tokenizer, _max_length):\n",
    "\n",
    "  def tokenize_function(examples):\n",
    "    max_length = _max_length\n",
    "\n",
    "    # Set pad token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    if \"question\" in examples and \"answer\" in examples:\n",
    "      text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "    elif \"input\" in examples and \"output\" in examples:\n",
    "      text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "    else:\n",
    "      text = examples[\"text\"][0]\n",
    "\n",
    "    # Run tokenizer on all the text (the input and the output)\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "\n",
    "        # Return tensors in a numpy array (other options are pytorch or tf objects)\n",
    "        return_tensors=\"np\",\n",
    "\n",
    "        # Padding type is to pad to the longest sequence in the batch (other option is to a certain max length, or no padding)\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    # Calculate max length\n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        max_length\n",
    "    )\n",
    "\n",
    "    if tokenized_inputs[\"input_ids\"].shape[1] > max_length:\n",
    "        logger.warn(\n",
    "            f\"Truncating input from {tokenized_inputs['input_ids'].shape[1]} to {max_length}\"\n",
    "        )\n",
    "\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"]\n",
    "\n",
    "    return tokenized_inputs\n",
    "  return tokenize_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e2b1013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package not installed\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05623ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
